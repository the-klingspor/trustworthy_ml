{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Trustworthy Machine Learning**\n",
        "### Winter Semester 2022-2023\n",
        "### Lecturer: Seong Joon Oh\n",
        "### **Exercise 0 -- Preliminary Materials**\n",
        "\n",
        "---\n",
        "\n",
        "Student name <input> \n",
        "\n",
        "Student email <input> \n",
        "\n",
        "---\n",
        "\n",
        "#### **Submission deadline: 27/10/2022 at 23:59.**\n",
        "\n",
        "Welcome to the Trustworthy Machine Learning course! TML is an advanced course that assumes some basic knowledge of machine learning and deep learning. This zeroth exercise will test your prerequisite knowledge and skills. \n",
        "\n",
        "#### **Policy for the zeroth exercise**\n",
        "This exercise is the **only individual exercise** in our course. The rest of the exercises in our course will be submitted and graded per group. The purpose of this zeroth exercise is to ensure that individual members of each group are sufficiently committed. As such, **we will enrol only the students who submit the zeroth exercise**. We will only accept solutions with the **minimal passing grade 30/100 points** to make sure that students do not submit empty work. The grade for the zeroth exercise **does not count towards the final grade**. The main purpose of this exercise is to selectively enrol motivated students and for you to evaluate your own readiness for the course. \n",
        "\n",
        "\n",
        "####**Submission**\n",
        "Follow the below four steps.\n",
        "\n",
        "(1) Copy this colab file to your local gdrive;\n",
        "\n",
        "`File > Save a copy in Drive`\n",
        "\n",
        "(2) Work on the solution on your local copy;\n",
        "\n",
        "(3) Pin the version for submission in history;\n",
        "\n",
        "`Click on \"All changes saved\" or \"Last saved at XX:XX AM/PM\" next to the drop-down menus at the top > Select version to submit > Click on three vertical dots (vertical ellipsis) > Rename > Write \"Submission\" `\n",
        "\n",
        "(4) Share your local colab with `stai.there@gmail.com` before the deadline.\n",
        "\n",
        "`Click on \"Share\" at the top-right corner > Put stai.there@gmail.com in \"Add people and groups\" > Give the \"Viewer\" right and tick on \"Notify people\" > Click send.`\n",
        "\n",
        "Note that we are able to see the edit history with time stamps, so please ensure that you stop working on your notebook before the deadline."
      ],
      "metadata": {
        "id": "QFWnwGiwu-EP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **0.1 Multivariate Calculus (10 + 10 + 5 = 25 points)**\n",
        "\n",
        "Let $f\\in\\mathbb{R}^C$ be a vector with dimension $C$, equal to the number of classes. Let $Y\\in\\{1,\\cdots,C\\}$ be the corresponding ground-truth label. We define the softmax-cross-entropy loss as follows:\n",
        "\\begin{equation}\n",
        "    \\mathcal{L} = -\\sum_{j=1}^C \\delta_{jY} \\log \\frac{\\exp{f_j}}{\\sum_k \\exp{f_k}}\n",
        "\\end{equation}\n",
        "where $\\delta_{ab}$ is the Kronecker Delta:\n",
        "\\begin{equation}\n",
        "    \\delta_{ab} = \n",
        "    \\begin{cases}\n",
        "        1\\quad \\text{if $a=b$;} \\\\\n",
        "        0\\quad \\text{otherwise;}\n",
        "    \\end{cases}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "FmGAkVwBwCDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Compute the gradient $\\frac{\\partial \\mathcal{L}}{\\partial f_c}$ for $c\\in\\{1,\\cdots,C\\}$. It may be helpful to introduce the substitution $p_j:=\\frac{\\exp{f_j}}{\\sum_k \\exp{f_k}}$. **(10 points)**"
      ],
      "metadata": {
        "id": "KZ0YTixTj8nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WRITE YOUR SOLUTION HERE**"
      ],
      "metadata": {
        "id": "JaQ1fkSBkJZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $\\{(X_i, Y_i)\\}_{i=1}^N$ be data samples with $X\\in\\mathbb{R}^D$ (e.g. images) and $Y\\in\\{1,\\cdots,C\\}$ (e.g. labels). Let  $f:\\mathbb{R}^I\\rightarrow\\mathbb{R}^C$ be a two-layer neural network of the following architecture\n",
        "\\begin{equation}\n",
        "    f(X;W,V) = V\\cdot \\sigma(W\\cdot X)\n",
        "\\end{equation}\n",
        "where $W\\in\\mathbb{R}^{H\\times D}$ maps $X$ to a hidden space $\\mathbb{R}^H$, $\\sigma$ is the element-wise ReLU activation function $\\sigma(x)=\\max\\{0,x\\}$, and $V\\in\\mathbb{R}^{C\\times H}$ maps a hidden representation to the output space $\\mathbb{R}^C$. Now, plug in our two-layer neural network to the $f$ in the softmax-cross-entropy loss:\n",
        "\\begin{equation}\n",
        "    \\mathcal{L}(W,V) = -\\sum_{j=1}^C \\delta_{jY} \\log \\frac{\\exp{f_j(X;W,V)}}{\\sum_k \\exp{f_k(X;W,V)}}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "DV6YVDILkncs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Compute the gradient $\\frac{\\partial \\mathcal{L}}{\\partial V_{ch}}$ for $c\\in\\{1,\\cdots,C\\}$ and $h\\in\\{1,\\cdots,H\\}$ using the answer to (a) and chain rule. **(10 points)**"
      ],
      "metadata": {
        "id": "BImL0fBMkxZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WRITE YOUR SOLUTION HERE**"
      ],
      "metadata": {
        "id": "4fNQiEYwk2WE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Compute the gradient $\\frac{\\partial \\mathcal{L}}{\\partial W_{hd}}$ for $h\\in\\{1,\\cdots,H\\}$ and $d\\in\\{1,\\cdots,D\\}$ using the answer to (b) and chain rule. **(5 points)**"
      ],
      "metadata": {
        "id": "dJu-FZ2xlDFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WRITE YOUR SOLUTION HERE**"
      ],
      "metadata": {
        "id": "YfpzFic3lGsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **0.2 Generalisation (5 + 5 + 5 + 5 + 5 = 25 points)**\n"
      ],
      "metadata": {
        "id": "G0XM7OHjw97x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) What is the role of training, validation, and test splits of a dataset for machine learning? **(5 points)**"
      ],
      "metadata": {
        "id": "l-mbINjflcUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WRITE YOUR SOLUTION HERE**"
      ],
      "metadata": {
        "id": "RdJXLBo3lc60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider an image classification task, where $X\\in\\mathbb{R}^D$ is an input image and $Y\\in\\{1,\\cdots,C\\}$ is the corresponding label following $p(X,Y)$. Let $\\mathcal{D}^\\text{tr}=\\{(X_i,Y_i)\\}_{i=1}^N$ and $\\mathcal{D}^\\text{te}=\\{(X_i,Y_i)\\}_{i=N+1}^{N+M}$ be training and test samples, respectively, that are IID-sampled from $p(X,Y)$. Let $f_\\theta$ be a model trained on $\\mathcal{D}^\\text{tr}$. Write $f_\\theta(c,X)\\in[0,1]$ for the predicted probability that image $X$ belongs to class $c$."
      ],
      "metadata": {
        "id": "12uNvbjqlin6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Write down the equations for the training- and test-set accuracies of $p_\\theta$. **(5 points)**"
      ],
      "metadata": {
        "id": "L528qOn1ljfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WRITE YOUR SOLUTION HERE**"
      ],
      "metadata": {
        "id": "RDsZszitlnji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Explain what it means to say that $f_\\theta$ \"generalises well\" and introduce a quantitative metric for this. **(5 points)**"
      ],
      "metadata": {
        "id": "X83E1DdjlpTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WRITE YOUR SOLUTION HERE**"
      ],
      "metadata": {
        "id": "plqcogLGltw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Explain the concept of overfitting and underfitting. **(5 points)**\n"
      ],
      "metadata": {
        "id": "asxIvGjjlyqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WRITE YOUR SOLUTION HERE**"
      ],
      "metadata": {
        "id": "sfPBkF-UlzzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Explain the respective solutions for overfitting and underfitting. **(5 points)**"
      ],
      "metadata": {
        "id": "0Ma7ifHdl20n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WRITE YOUR SOLUTION HERE**"
      ],
      "metadata": {
        "id": "xAZchYMdl40v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **0.3 MNIST Case-study (15 + 15 + 10 + 10 = 50 points)**\n",
        "\n",
        "*This exercise is based on the public code at https://colab.research.google.com/github/skorch-dev/skorch/blob/master/notebooks/MNIST.ipynb (BSD 3-Clause License)*."
      ],
      "metadata": {
        "id": "fXn5gZIWwd7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! [ ! -z \"$COLAB_GPU\" ] && pip install torch scikit-learn==0.20.* skorch"
      ],
      "metadata": {
        "id": "dqzjwU0JzFTl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "949971ca-8628-4d55-c527-b22ecc3a9fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: scikit-learn==0.20.* in /usr/local/lib/python3.7/dist-packages (0.20.4)\n",
            "Requirement already satisfied: skorch in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.20.*) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.20.*) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.8.10)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import logging"
      ],
      "metadata": {
        "id": "hPp10JADzaVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data\n",
        "\n",
        "The following code downloads MNIST. It takes 2-3 minutes."
      ],
      "metadata": {
        "id": "OjddOGaWzhsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = fetch_openml('mnist_784', cache=True)"
      ],
      "metadata": {
        "id": "Aht68ourzcik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist.data.shape"
      ],
      "metadata": {
        "id": "fRi_gw4FzeWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6e7b9a-7819-4c56-9155-5014f28d489d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Data\n",
        "\n",
        "Each image of the MNIST dataset is encoded in a 784 dimensional vector, representing a 28 x 28 pixel image. Each pixel has a value between 0 and 255, corresponding to the grey-value of a pixel.<br />\n",
        "The above ```featch_mldata``` method to load MNIST returns ```data``` and ```target``` as ```uint8``` which we convert to ```float32``` and ```int64``` respectively."
      ],
      "metadata": {
        "id": "QcdCLOvyzoog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = mnist.data.astype('float32')\n",
        "y = mnist.target.astype('int64')"
      ],
      "metadata": {
        "id": "5HOEqw3rzoL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid big weights that deal with the pixel values from between [0, 255], we scale `X` down. A commonly used range is [0, 1]."
      ],
      "metadata": {
        "id": "PwRg0az8zt4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X /= 255.0"
      ],
      "metadata": {
        "id": "VaRVxh_Kzulg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.min(), X.max()"
      ],
      "metadata": {
        "id": "Rm0A8CD6zxGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f2943f-08bd-475e-a499-f81fec7ec037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Train-Val-Test Splits (15 points)\n",
        "\n",
        "Let's split the given MNIST data (70000 samples) into 3 partitions: train, val, and test. Complete the function `train_test_split` **(15 points)**."
      ],
      "metadata": {
        "id": "BtrbiHPrD9yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(X, y, val_size, test_size, random_seed=None):\n",
        "  \"\"\"\n",
        "  Given a labelled dataset in memory, return two disjoint partitions, train and test, randomly split according to the given test_size.\n",
        "  For replicability, make sure that the same random_seed returns the same split of the dataset.\n",
        "  Args:\n",
        "    X: numpy.ndarray of shape (N, D) and type np.float32. N is the number of sample instances and D is the dimensionality of input features.\n",
        "    y: numpy.ndarray of shape (N,) and type np.int64.\n",
        "    val_size: float value between 0 and 1. Controls the ratio of val samples.\n",
        "    test_size: float value between 0 and 1. Controls the ratio of test samples.\n",
        "    random_seed: int or None. If it is int, set the random seed to ensure replicability. If it is None, do not set the random seed.\n",
        "  Returns:\n",
        "    dict of\n",
        "      X_train: numpy.ndarray of shape (N - L - M, D) and type np.float32.\n",
        "      y_train: numpy.ndarray of shape (N - L - M,) and type np.int64.\n",
        "      X_val: numpy.ndarray of shape (L, D) and type np.float32. L = int(N * val_size).\n",
        "      y_val: numpy.ndarray of shape (L,) and type np.int64.\n",
        "      X_test: numpy.ndarray of shape (M, D) and type np.float32. M = int(N * test_size).\n",
        "      y_test: numpy.ndarray of shape (M,) and type np.int64.\n",
        "  \"\"\"\n",
        "  #### >>>> PUT YOUR SOLUTION HERE <<<< 15 points\n",
        "\n",
        "  #### >>>> ENT OF YOUR SOLUTION <<<<\n",
        "  return {\n",
        "      \"X_train\": X_train,\n",
        "      \"y_train\": y_train,\n",
        "      \"X_val\": X_val,\n",
        "      \"y_val\": y_val,\n",
        "      \"X_test\": X_test,\n",
        "      \"y_test\": y_test,\n",
        "  }"
      ],
      "metadata": {
        "id": "UvhnvEe10wlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define test functions."
      ],
      "metadata": {
        "id": "Fll3Ze71E-46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_diff_matrix(mat1, mat2, max_samples):\n",
        "  \"\"\"\n",
        "  Compute the pairwise L1 distance among the feature vectors in mat1 and mat2.\n",
        "  Args:\n",
        "    mat1: array of shape (N, D) where D is the feature dimension.\n",
        "    mat2: array of shape (M, D)\n",
        "    max_samples: for the interest of RAM usage, we may restrict the max number of samples\n",
        "  Returns:\n",
        "    dist_mat: array of shape (N, M) where N <- min(max_samples, N) and M <- min(max_samples, M)\n",
        "  \"\"\"\n",
        "  num_samples1 = min(max_samples, len(mat1))\n",
        "  num_samples2 = min(max_samples, len(mat2))\n",
        "  diff_matrix = np.abs(\n",
        "      np.expand_dims(mat1[:num_samples1], axis=0) # (1, N, D)\n",
        "    - np.expand_dims(mat2[:num_samples2], axis=1) # (M, 1, D)\n",
        "  ).sum(axis=2)\n",
        "  return diff_matrix\n",
        "\n",
        "\n",
        "def train_test_split_test(X, y):\n",
        "  print(f\"Checking train-val-test split sizes.\")\n",
        "  data_splits = train_test_split(X, y, val_size=0.2, test_size=0.5, random_seed=None)\n",
        "  assert(len(data_splits[\"X_train\"]) == 21000)\n",
        "  assert(len(data_splits[\"X_val\"]) == 14000)\n",
        "  assert(len(data_splits[\"X_test\"]) == 35000)\n",
        "  assert(len(data_splits[\"y_train\"]) == 21000)\n",
        "  assert(len(data_splits[\"y_val\"]) == 14000)\n",
        "  assert(len(data_splits[\"y_test\"]) == 35000)\n",
        "  data_splits = train_test_split(X, y, val_size=0.1, test_size=0.8, random_seed=None)\n",
        "  assert(len(data_splits[\"X_train\"]) == 7000)\n",
        "  assert(len(data_splits[\"X_val\"]) == 7000)\n",
        "  assert(len(data_splits[\"X_test\"]) == 56000)\n",
        "  assert(len(data_splits[\"y_train\"]) == 7000)\n",
        "  assert(len(data_splits[\"y_val\"]) == 7000)\n",
        "  assert(len(data_splits[\"y_test\"]) == 56000)\n",
        "\n",
        "  print(f\"Checking train-test split purity.\")\n",
        "  diff_matrix_tr_te = compute_diff_matrix(mat1=data_splits[\"X_train\"],\n",
        "                                          mat2=data_splits[\"X_test\"],\n",
        "                                          max_samples=1000)\n",
        "  assert(diff_matrix_tr_te.min() > 0)\n",
        "  diff_matrix_tr_val = compute_diff_matrix(mat1=data_splits[\"X_train\"],\n",
        "                                           mat2=data_splits[\"X_val\"],\n",
        "                                           max_samples=1000)\n",
        "  assert(diff_matrix_tr_val.min() > 0)\n",
        "  diff_matrix_te_val = compute_diff_matrix(mat1=data_splits[\"X_test\"],\n",
        "                                           mat2=data_splits[\"X_val\"],\n",
        "                                           max_samples=1000)\n",
        "  assert(diff_matrix_te_val.min() > 0)\n",
        "\n",
        "  print(f\"Checking whether random seed is working.\")\n",
        "  data_splits1 = train_test_split(X, y, val_size=0.1, test_size=0.3, random_seed=65)\n",
        "  data_splits2 = train_test_split(X, y, val_size=0.1, test_size=0.3, random_seed=65)\n",
        "  data_splits3 = train_test_split(X, y, val_size=0.1, test_size=0.3, random_seed=66)\n",
        "  data_splits4 = train_test_split(X, y, val_size=0.1, test_size=0.3, random_seed=None)\n",
        "  data_splits5 = train_test_split(X, y, val_size=0.1, test_size=0.3, random_seed=None)\n",
        "  \n",
        "  assert((data_splits1[\"X_train\"]==data_splits2[\"X_train\"]).all())\n",
        "  assert((data_splits2[\"X_train\"]!=data_splits3[\"X_train\"]).any())\n",
        "  assert((data_splits4[\"X_train\"]!=data_splits5[\"X_train\"]).any())"
      ],
      "metadata": {
        "id": "upTtGa8HE7eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your solution by running the test function below."
      ],
      "metadata": {
        "id": "CddjJDafllCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_test_split_test(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS1WumsDkHP-",
        "outputId": "f1509eb1-b85f-4591-edfe-bd01c87e580d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking train-val-test split sizes.\n",
            "Checking train-test split purity.\n",
            "Checking whether random seed is working.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_splits = train_test_split(X, y, val_size=0.1, test_size=0.1, random_seed=42)"
      ],
      "metadata": {
        "id": "bSl_We9Tz0Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in data_splits:\n",
        "  print(f\"{key} shape: {data_splits[key].shape}\")"
      ],
      "metadata": {
        "id": "_vTSvDQZz5dU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7722455a-9bdd-464d-ef14-99254bfbf877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (56000, 784)\n",
            "y_train shape: (56000,)\n",
            "X_val shape: (7000, 784)\n",
            "y_val shape: (7000,)\n",
            "X_test shape: (7000, 784)\n",
            "y_test shape: (7000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_example(X, y, num_samples=10):\n",
        "    \"\"\"Plot the first N images and their labels in a row.\"\"\"\n",
        "    for i, (img, y) in enumerate(zip(X[:num_samples].reshape(num_samples, 28, 28), y[:num_samples])):\n",
        "        plt.subplot(1, num_samples, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(y)"
      ],
      "metadata": {
        "id": "zgYMhzTBz9N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_example(data_splits[\"X_train\"], data_splits[\"y_train\"])"
      ],
      "metadata": {
        "id": "6SSITy7R0ALB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "9c3e9ace-2c18-4f7c-9ff0-08c3a2cd8f38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define an MLP Model\n",
        "Simple, fully connected neural network with one hidden layer. Input layer has 784 dimensions (28x28), hidden layer has 98 (= 784 / 8) and output layer 10 neurons, representing digits 0 - 9."
      ],
      "metadata": {
        "id": "5TOSzJD-06fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "4SJhz-4606JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "xnExi2at0-9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AfA6jDFX4fEG",
        "outputId": "dbb5e335-4983-4c30-ad75-0bd85b88bc4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            output_dim,\n",
        "            dropout=0.5,\n",
        "    ):\n",
        "        super(MLP, self).__init__()\n",
        "        print(f\"input_dim={input_dim}, hidden_dim={hidden_dim}, output_dim={output_dim}\")\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
        "        self.output = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, X, **kwargs):\n",
        "        X = F.relu(self.hidden(X))\n",
        "        X = self.dropout(X)\n",
        "        X = F.softmax(self.output(X), dim=-1)\n",
        "        return X"
      ],
      "metadata": {
        "id": "evkViHmF1Gyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Loss Function (15 points)\n",
        "\n",
        "We train the model with the softmax-cross-entropy loss. The first part of the function is already there.\n",
        "```\n",
        "outputs = outputs - outputs.max(1, keepdim=True)[0]\n",
        "```\n",
        "This part ensures the numerical stability of the softmax operation, which is translation invariant.\n",
        "```\n",
        "labels_onehot = F.one_hot(labels, num_classes=10)\n",
        "```\n",
        "This part turns the integer labels into one-hot vectors.\n",
        "\n",
        "Complete the loss function **(15 points)**."
      ],
      "metadata": {
        "id": "q9ezQK5mEvvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxCrossEntropyLoss(nn.Module):\n",
        "  def forward(self, outputs, labels):\n",
        "    \"\"\"\n",
        "    Compute softmax cross-entropy loss.\n",
        "    Args:\n",
        "      outputs: Torch.Tensor of shape (B, C) and type float32.\n",
        "      labels: Torch.Tensor of shape (B,) and type long.\n",
        "    Returns:\n",
        "      loss: Torch.Tensor of scalar shape and type float32.\n",
        "    \"\"\"\n",
        "    outputs = outputs - outputs.max(1, keepdim=True)[0]\n",
        "    labels_onehot = F.one_hot(labels, num_classes=10) # shape (B, C) type bool\n",
        "\n",
        "    #### >>>> PUT YOUR SOLUTION HERE <<<< 15 points\n",
        "\n",
        "    #### >>>> ENT OF YOUR SOLUTION <<<<\n",
        "    return loss"
      ],
      "metadata": {
        "id": "TSmFhsy1Ey5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Solver\n",
        "\n",
        "A programming framework that is often used to manage model training, inference, and evaluation is the Solver object. This is advantageous because such activities around a model share common data and methods: \n",
        "- Model (torch.nn.Module)\n",
        "- Dataset (torch.utils.data.Dataset)\n",
        "\n",
        "We have defined the Solver class for you. You may find this framework useful for managing ML code later on."
      ],
      "metadata": {
        "id": "ey7IKKQrGU1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Solver(object):\n",
        "  def __init__(self, data_splits, model, criterion, config):\n",
        "    self.config = config\n",
        "    self.model = model\n",
        "    self.criterion = criterion\n",
        "    self.optimizer = optim.SGD(self.model.parameters(),\n",
        "                               lr=config[\"lr\"],\n",
        "                               momentum=config[\"momentum\"])\n",
        "    self.data_splits = data_splits\n",
        "\n",
        "  def load_dataloader(self, split, role):\n",
        "    if role == \"eval\":\n",
        "      batch_size = config[\"eval_batch_size\"]\n",
        "      shuffle = True\n",
        "    elif role == \"train\":\n",
        "      batch_size = config[\"batch_size\"]\n",
        "      shuffle = False\n",
        "    else:\n",
        "      raise ValueError(f\"Unknown role name {role}.\")\n",
        "    \n",
        "    return torch.utils.data.DataLoader(\n",
        "      torch.utils.data.TensorDataset(\n",
        "          torch.from_numpy(self.data_splits[f\"X_{split}\"]),\n",
        "          torch.from_numpy(self.data_splits[f\"y_{split}\"])\n",
        "      ), \n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      num_workers=config[\"num_workers\"])\n",
        "\n",
        "  def fit_one_batch(self, inputs, labels):\n",
        "    self.optimizer.zero_grad()\n",
        "    outputs = self.model(inputs)\n",
        "    loss = self.criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    return loss\n",
        "\n",
        "  def fit_one_epoch(self, epoch_idx):\n",
        "    loss_epoch = 0.0\n",
        "    dataloader = self.load_dataloader(split=\"train\", role=\"train\")\n",
        "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "      loss = self.fit_one_batch(inputs, labels)\n",
        "      loss_epoch += loss.item()\n",
        "    loss_epoch /= batch_idx + 1\n",
        "    print(f\"train_loss {loss_epoch: 2.3f}\", end=\"\\t\")\n",
        "\n",
        "  def fit(self, evaluate_on):\n",
        "    for epoch_idx in range(self.config[\"num_epochs\"]):\n",
        "      print(f\"epoch {epoch_idx + 1:03d}\", end=\"\\t\")\n",
        "      self.fit_one_epoch(epoch_idx)\n",
        "      accuracy = self.evaluate(evaluate_on)\n",
        "      print(f\"val_accuracy: {accuracy: 2.3f}%\", end=\"\\t\")\n",
        "      print()\n",
        "\n",
        "  def evaluate(self, evaluate_on):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    dataloader = self.load_dataloader(split=evaluate_on, role=\"eval\")\n",
        "    with torch.no_grad():\n",
        "      for (inputs, labels) in dataloader:\n",
        "        outputs = self.model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total"
      ],
      "metadata": {
        "id": "02p-nOmemN5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model\n",
        "\n",
        "Initialise your solver with some initial config values and train the model"
      ],
      "metadata": {
        "id": "t1ve3U8SHtF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "  # Critical configs that affect the trained model\n",
        "  \"batch_size\": 512,\n",
        "  \"lr\": 0.001,\n",
        "  \"momentum\": 0.9,\n",
        "  \"num_epochs\": 50,\n",
        "  # Configs that do not influence the trained model\n",
        "  \"num_workers\": 0,\n",
        "  \"eval_batch_size\": 128,\n",
        "}\n",
        "\n",
        "input_dim = data_splits[\"X_train\"].shape[1]\n",
        "\n",
        "solver = Solver(\n",
        "    data_splits=data_splits,\n",
        "    model=MLP(\n",
        "      input_dim=input_dim,\n",
        "      hidden_dim=int(input_dim/8),\n",
        "      output_dim=10,\n",
        "    ),\n",
        "    criterion=SoftmaxCrossEntropyLoss(),\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "solver.fit(evaluate_on=\"val\")\n",
        "accuracy = solver.evaluate(evaluate_on=\"val\")\n",
        "print(f\"Final accuracy: {accuracy:.3f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lgPmLNgm2ax",
        "outputId": "be5c36cf-7533-4740-c22a-3f9df58d29c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_dim=784, hidden_dim=98, output_dim=10\n",
            "epoch 001\ttrain_loss  2.302\tval_accuracy:  12.429%\t\n",
            "epoch 002\ttrain_loss  2.300\tval_accuracy:  17.443%\t\n",
            "epoch 003\ttrain_loss  2.297\tval_accuracy:  22.629%\t\n",
            "epoch 004\ttrain_loss  2.295\tval_accuracy:  28.143%\t\n",
            "epoch 005\ttrain_loss  2.292\tval_accuracy:  32.429%\t\n",
            "epoch 006\ttrain_loss  2.288\tval_accuracy:  35.186%\t\n",
            "epoch 007\ttrain_loss  2.284\tval_accuracy:  37.343%\t\n",
            "epoch 008\ttrain_loss  2.279\tval_accuracy:  39.243%\t\n",
            "epoch 009\ttrain_loss  2.273\tval_accuracy:  40.086%\t\n",
            "epoch 010\ttrain_loss  2.265\tval_accuracy:  39.871%\t\n",
            "epoch 011\ttrain_loss  2.255\tval_accuracy:  39.029%\t\n",
            "epoch 012\ttrain_loss  2.242\tval_accuracy:  38.571%\t\n",
            "epoch 013\ttrain_loss  2.228\tval_accuracy:  39.814%\t\n",
            "epoch 014\ttrain_loss  2.212\tval_accuracy:  41.400%\t\n",
            "epoch 015\ttrain_loss  2.196\tval_accuracy:  42.971%\t\n",
            "epoch 016\ttrain_loss  2.177\tval_accuracy:  43.429%\t\n",
            "epoch 017\ttrain_loss  2.157\tval_accuracy:  45.100%\t\n",
            "epoch 018\ttrain_loss  2.136\tval_accuracy:  46.786%\t\n",
            "epoch 019\ttrain_loss  2.115\tval_accuracy:  48.957%\t\n",
            "epoch 020\ttrain_loss  2.093\tval_accuracy:  49.643%\t\n",
            "epoch 021\ttrain_loss  2.072\tval_accuracy:  51.843%\t\n",
            "epoch 022\ttrain_loss  2.053\tval_accuracy:  53.257%\t\n",
            "epoch 023\ttrain_loss  2.035\tval_accuracy:  53.171%\t\n",
            "epoch 024\ttrain_loss  2.018\tval_accuracy:  55.557%\t\n",
            "epoch 025\ttrain_loss  2.002\tval_accuracy:  58.029%\t\n",
            "epoch 026\ttrain_loss  1.987\tval_accuracy:  58.243%\t\n",
            "epoch 027\ttrain_loss  1.972\tval_accuracy:  60.471%\t\n",
            "epoch 028\ttrain_loss  1.958\tval_accuracy:  61.957%\t\n",
            "epoch 029\ttrain_loss  1.944\tval_accuracy:  62.514%\t\n",
            "epoch 030\ttrain_loss  1.930\tval_accuracy:  63.543%\t\n",
            "epoch 031\ttrain_loss  1.921\tval_accuracy:  64.057%\t\n",
            "epoch 032\ttrain_loss  1.910\tval_accuracy:  65.243%\t\n",
            "epoch 033\ttrain_loss  1.901\tval_accuracy:  65.771%\t\n",
            "epoch 034\ttrain_loss  1.892\tval_accuracy:  65.857%\t\n",
            "epoch 035\ttrain_loss  1.885\tval_accuracy:  66.257%\t\n",
            "epoch 036\ttrain_loss  1.876\tval_accuracy:  67.057%\t\n",
            "epoch 037\ttrain_loss  1.869\tval_accuracy:  67.000%\t\n",
            "epoch 038\ttrain_loss  1.863\tval_accuracy:  67.357%\t\n",
            "epoch 039\ttrain_loss  1.857\tval_accuracy:  67.857%\t\n",
            "epoch 040\ttrain_loss  1.850\tval_accuracy:  68.200%\t\n",
            "epoch 041\ttrain_loss  1.845\tval_accuracy:  68.843%\t\n",
            "epoch 042\ttrain_loss  1.839\tval_accuracy:  69.857%\t\n",
            "epoch 043\ttrain_loss  1.832\tval_accuracy:  70.771%\t\n",
            "epoch 044\ttrain_loss  1.825\tval_accuracy:  72.057%\t\n",
            "epoch 045\ttrain_loss  1.817\tval_accuracy:  73.400%\t\n",
            "epoch 046\ttrain_loss  1.811\tval_accuracy:  74.014%\t\n",
            "epoch 047\ttrain_loss  1.805\tval_accuracy:  73.743%\t\n",
            "epoch 048\ttrain_loss  1.799\tval_accuracy:  74.671%\t\n",
            "epoch 049\ttrain_loss  1.794\tval_accuracy:  74.471%\t\n",
            "epoch 050\ttrain_loss  1.790\tval_accuracy:  74.643%\t\n",
            "Final accuracy: 75.143%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning (10 points)\n",
        "\n",
        "Now, find a config set (`batch_size`, `lr`, `momentum`) that returns a **validation-set accuracy >= 95%** at any epoch <=50 **(10 points)**.\n",
        "\n",
        "**You may not change the training data.**"
      ],
      "metadata": {
        "id": "VJwHPRhGD12W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### >>>> PUT YOUR SOLUTION HERE <<<< 10 points\n",
        "config = {\n",
        "  # Critical configs that affect the trained model\n",
        "  \"batch_size\": 512,\n",
        "  \"lr\": 0.001,\n",
        "  \"momentum\": 0.9,\n",
        "  \"num_epochs\": 50,\n",
        "  # Configs that do not influence the trained model\n",
        "  \"num_workers\": 0,\n",
        "  \"eval_batch_size\": 128,\n",
        "}\n",
        "#### >>>> ENT OF YOUR SOLUTION <<<<\n",
        "\n",
        "input_dim = data_splits[\"X_train\"].shape[1]\n",
        "\n",
        "solver = Solver(\n",
        "    data_splits=data_splits,\n",
        "    model=MLP(\n",
        "      input_dim=input_dim,\n",
        "      hidden_dim=int(input_dim/8),\n",
        "      output_dim=10,\n",
        "    ),\n",
        "    criterion=SoftmaxCrossEntropyLoss(),\n",
        "    config=config\n",
        ")\n",
        "\n",
        "solver.fit(evaluate_on=\"val\")\n",
        "accuracy = solver.evaluate(evaluate_on=\"val\")\n",
        "print(f\"Final accuracy: {accuracy:.3f}%\")"
      ],
      "metadata": {
        "id": "py-hDNi4Jcup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a CNN Model\n",
        "We may take advantage of the image structure to enjoy more efficient usage of model parameters. CNNs assume that parameters can be devoted to model the composition of nearby pixels in an image, rather than any long-range dependence. We define a CNN model for you. This is a LeNet structure."
      ],
      "metadata": {
        "id": "9ok_1tvBIaEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, dropout=0.5):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.conv2_drop = nn.Dropout2d(p=dropout)\n",
        "        self.fc1 = nn.Linear(1600, 100) # 1600 = number channels * width * height\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "        self.fc1_drop = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(-1, 1, 28, 28)\n",
        "        x = torch.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = torch.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        \n",
        "        # flatten over channel, height and width = 1600\n",
        "        x = x.view(-1, x.size(1) * x.size(2) * x.size(3))\n",
        "        \n",
        "        x = torch.relu(self.fc1_drop(self.fc1(x)))\n",
        "        x = torch.softmax(self.fc2(x), dim=-1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RL0AVctW1vxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a CNN Model"
      ],
      "metadata": {
        "id": "Oifjml3_QaG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "  # Critical configs that affect the trained model\n",
        "  \"batch_size\": 128,\n",
        "  \"lr\": 0.1,\n",
        "  \"momentum\": 0.9,\n",
        "  \"num_epochs\": 10,\n",
        "  # Configs that do not influence the trained model\n",
        "  \"num_workers\": 0,\n",
        "  \"eval_batch_size\": 128,\n",
        "}\n",
        "\n",
        "input_dim = data_splits[\"X_train\"].shape[1]\n",
        "\n",
        "solver = Solver(\n",
        "    data_splits=data_splits,\n",
        "    model=CNN(),\n",
        "    criterion=SoftmaxCrossEntropyLoss(),\n",
        "    config=config\n",
        ")\n",
        "\n",
        "solver.fit(evaluate_on=\"val\")\n",
        "accuracy = solver.evaluate(evaluate_on=\"val\")\n",
        "print(f\"Final accuracy: {accuracy:.3f}%\")"
      ],
      "metadata": {
        "id": "8FGYIfnO1xd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Report (10 points)\n",
        "\n",
        "We want to answer the research question: Is MLP or CNN more advantageous for MNIST digit classification task?\n",
        "\n",
        "Make an argument below, based on empirical evidence, which architecture is \"better\"?\n",
        "\n",
        "For answering this question, consider the following aspects:\n",
        "- Accuracy, indeed, but also\n",
        "- Computational complexity\n",
        "  - Space and time\n",
        "  - Training and inference\n",
        "- Fairness of hyperparameter tuning and model choice\n",
        "- Error bar of accuracy\n",
        "\n",
        "Feel free to run additional model training experiments to support your argument. **(10 points)**"
      ],
      "metadata": {
        "id": "upcUH5k1L5Yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WRITE YOUR SOLUTION HERE**\n"
      ],
      "metadata": {
        "id": "OLhHQmChOIrL"
      }
    }
  ]
}