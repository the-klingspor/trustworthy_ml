{"cells":[{"cell_type":"markdown","metadata":{"id":"QFWnwGiwu-EP"},"source":["# **Trustworthy Machine Learning**\n","### Winter Semester 2022-2023\n","### Lecturer: Seong Joon Oh\n","### **Exercise 0 -- Preliminary Materials**\n","\n","---\n","\n","Student name: Balint Mucsanyi\n","\n","Student email: balint.mucsanyi@student.uni-tuebingen.de\n","\n","---\n","\n","#### **Submission deadline: 27/10/2022 at 23:59.**\n","\n","Welcome to the Trustworthy Machine Learning course! TML is an advanced course that assumes some basic knowledge of machine learning and deep learning. This zeroth exercise will test your prerequisite knowledge and skills. \n","\n","#### **Policy for the zeroth exercise**\n","This exercise is the **only individual exercise** in our course. The rest of the exercises in our course will be submitted and graded per group. The purpose of this zeroth exercise is to ensure that individual members of each group are sufficiently committed. As such, **we will enrol only the students who submit the zeroth exercise**. We will only accept solutions with the **minimal passing grade 30/100 points** to make sure that students do not submit empty work. The grade for the zeroth exercise **does not count towards the final grade**. The main purpose of this exercise is to selectively enrol motivated students and for you to evaluate your own readiness for the course. \n","\n","\n","####**Submission**\n","Follow the below four steps.\n","\n","(1) Copy this colab file to your local gdrive;\n","\n","`File > Save a copy in Drive`\n","\n","(2) Work on the solution on your local copy;\n","\n","(3) Pin the version for submission in history;\n","\n","`Click on \"All changes saved\" or \"Last saved at XX:XX AM/PM\" next to the drop-down menus at the top > Select version to submit > Click on three vertical dots (vertical ellipsis) > Rename > Write \"Submission\" `\n","\n","(4) Share your local colab with `stai.there@gmail.com` before the deadline.\n","\n","`Click on \"Share\" at the top-right corner > Put stai.there@gmail.com in \"Add people and groups\" > Give the \"Viewer\" right and tick on \"Notify people\" > Click send.`\n","\n","Note that we are able to see the edit history with time stamps, so please ensure that you stop working on your notebook before the deadline."]},{"cell_type":"markdown","metadata":{"id":"FmGAkVwBwCDu"},"source":["## **0.1 Multivariate Calculus (10 + 10 + 5 = 25 points)**\n","\n","Let $f\\in\\mathbb{R}^C$ be a vector with dimension $C$, equal to the number of classes. Let $Y\\in\\{1,\\cdots,C\\}$ be the corresponding ground-truth label. We define the softmax-cross-entropy loss as follows:\n","\\begin{equation}\n","    \\mathcal{L} = -\\sum_{j=1}^C \\delta_{jY} \\log \\frac{\\exp{f_j}}{\\sum_k \\exp{f_k}}\n","\\end{equation}\n","where $\\delta_{ab}$ is the Kronecker Delta:\n","\\begin{equation}\n","    \\delta_{ab} = \n","    \\begin{cases}\n","        1\\quad \\text{if $a=b$;} \\\\\n","        0\\quad \\text{otherwise;}\n","    \\end{cases}\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"KZ0YTixTj8nD"},"source":["(a) Compute the gradient $\\frac{\\partial \\mathcal{L}}{\\partial f_c}$ for $c\\in\\{1,\\cdots,C\\}$. It may be helpful to introduce the substitution $p_j:=\\frac{\\exp{f_j}}{\\sum_k \\exp{f_k}}$. **(10 points)**"]},{"cell_type":"markdown","metadata":{"id":"JaQ1fkSBkJZP"},"source":["$$\n","\\begin{align}\n","\\mathcal{L} &= -\\sum_{j = 1}^C \\delta_{jY}\\log \\frac{\\exp f_j}{\\sum_{k=1}^C \\exp f_k}\\\\\n","&= -\\log \\frac{\\exp f_Y}{\\sum_{k=1}^C \\exp f_k}\\\\\n","&= -\\left(\\log \\exp f_Y - \\log \\sum_{k=1}^C \\exp f_k\\right)\\\\\n","&= \\log \\sum_{k=1}^C \\exp f_k - f_Y\n","\\end{align}\n","$$\n","$\\forall c \\in \\{1, \\dots, C\\}:$\n","$$\n","\\begin{align}\n","\\frac{\\partial \\mathcal{L}}{\\partial f_c} &= \\frac{\\partial}{\\partial f_c}\\left(\\log \\sum_{k=1}^C \\exp f_k - f_Y\\right)\\\\\n","&= \\frac{1}{\\sum_{k=1}^C \\exp f_k} \\cdot \\sum_{k=1}^C \\exp f_k \\delta_{kc} - \\delta_{Yc}\\\\\n","&= \\frac{\\exp f_c}{\\sum_{k=1}^C \\exp f_k} - \\delta_{Yc}\\\\\n","&= \\text{softmax}(f)_c - \\delta_{Yc}.\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"DV6YVDILkncs"},"source":["Let $\\{(X_i, Y_i)\\}_{i=1}^N$ be data samples with $X\\in\\mathbb{R}^D$ (e.g. images) and $Y\\in\\{1,\\cdots,C\\}$ (e.g. labels). Let  $f:\\mathbb{R}^D\\rightarrow\\mathbb{R}^C$ be a two-layer neural network of the following architecture\n","\\begin{equation}\n","    f(X;W,V) = V\\cdot \\sigma(W\\cdot X)\n","\\end{equation}\n","where $W\\in\\mathbb{R}^{H\\times D}$ maps $X$ to a hidden space $\\mathbb{R}^H$, $\\sigma$ is the element-wise ReLU activation function $\\sigma(x)=\\max\\{0,x\\}$, and $V\\in\\mathbb{R}^{C\\times H}$ maps a hidden representation to the output space $\\mathbb{R}^C$. Now, plug in our two-layer neural network to the $f$ in the softmax-cross-entropy loss:\n","\\begin{equation}\n","    \\mathcal{L}(W,V) = -\\sum_{j=1}^C \\delta_{jY} \\log \\frac{\\exp{f_j(X;W,V)}}{\\sum_k \\exp{f_k(X;W,V)}}\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"BImL0fBMkxZq"},"source":["(b) Compute the gradient $\\frac{\\partial \\mathcal{L}}{\\partial V_{ch}}$ for $c\\in\\{1,\\cdots,C\\}$ and $h\\in\\{1,\\cdots,H\\}$ using the answer to (a) and chain rule. **(10 points)**"]},{"cell_type":"markdown","metadata":{"id":"4fNQiEYwk2WE"},"source":["$$\n","\\begin{align}\n","\\mathcal{L}(W, V) &= -\\sum_{j=1}^C \\delta_{jY}\\log \\frac{\\exp f_j(X; W, V)}{\\sum_{k=1}^C \\exp f_k(X; W, V)}\\\\\n","&\\overset{(a)}{=} \\log \\sum_{k=1}^C \\exp f_k(X; W, V) - f_Y(X; W, V)\n","\\end{align}\n","$$\n","$\\forall c \\in \\{1, \\dots, C\\}, h \\in \\{1, \\dots, H\\}:$\n","$$\\frac{\\partial \\mathcal{L}}{\\partial V_{ch}} = \\sum_{k=1}^C \\frac{\\partial \\mathcal{L}}{\\partial f_k}\\cdot \\frac{\\partial f_k}{\\partial V_{ch}}$$\n","Here we have\n","$$\n","\\begin{align}\n","\\frac{\\partial \\mathcal{L}}{\\partial f_k} \\overset{(a)}{=} \\frac{\\exp f_k}{\\sum_{q=1}^C \\exp f_q} - \\delta_{Yk}\n","\\end{align}\n","$$\n","and\n","$$\n","\\begin{align}\n","\\frac{\\partial f_k}{\\partial V_{ch}} &= \\frac{\\partial}{\\partial V_{ch}}\\left(V\\cdot \\sigma(W \\cdot X)\\right)_k\\\\\n","&= \\frac{\\partial}{\\partial V_{ch}}\\left(\\sum_{l=1}^H V_{kl}\\sigma(W\\cdot X)_l\\right)\\\\\n","&= \\sum_{l=1}^H \\delta_{kc}\\delta_{lh}\\sigma(W \\cdot X)_l\\\\\n","&= \\delta_{kc}\\sigma(W \\cdot X)_h\n","\\end{align}\n","$$\n","thus,\n","$$\n","\\begin{align}\n","\\frac{\\partial \\mathcal{L}}{\\partial V_{ch}} &= \\sum_{k=1}^C \\left(\\frac{\\exp f_k}{\\sum_{q=1}^C \\exp f_q} - \\delta_{Yk}\\right)\\delta_{kc}\\sigma(W \\cdot X)_h\\\\\n","&= \\left(\\frac{\\exp f_c}{\\sum_{q=1}^C \\exp f_q} - \\delta_{Yc}\\right)\\sigma(W\\cdot X)_h.\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"dJu-FZ2xlDFb"},"source":["(c) Compute the gradient $\\frac{\\partial \\mathcal{L}}{\\partial W_{hd}}$ for $h\\in\\{1,\\cdots,H\\}$ and $d\\in\\{1,\\cdots,D\\}$ using the answer to (b) and chain rule. **(5 points)**"]},{"cell_type":"markdown","metadata":{"id":"YfpzFic3lGsN"},"source":["$\\forall h \\in \\{1, \\dots, H\\}, d \\in \\{1, \\dots, D\\}:$\n","$$\\frac{\\partial \\mathcal{L}}{\\partial W_{hd}} = \\sum_{k=1}^C \\frac{\\partial \\mathcal{L}}{\\partial f_k}\\cdot \\frac{\\partial f_k}{\\partial W_{hd}}$$\n","Here we have\n","$$\n","\\begin{align}\n","\\frac{\\partial \\mathcal{L}}{\\partial f_k} \\overset{(a)}{=} \\frac{\\exp f_k}{\\sum_{q=1}^C \\exp f_q} - \\delta_{Yk}\n","\\end{align}\n","$$\n","and\n","$$\n","\\begin{align}\n","\\frac{\\partial f_k}{\\partial W_{hd}} &= \\frac{\\partial}{\\partial W_{hd}}\\left(V\\cdot \\sigma(W \\cdot X)\\right)_k\\\\\n","&= \\frac{\\partial}{\\partial W_{hd}} \\left(\\sum_{l=1}^H V_{kl} \\sigma(W\\cdot X)_l\\right)\\\\\n","&= \\sum_{l=1}^H V_{kl} \\frac{\\partial \\sigma(W \\cdot X)_l}{\\partial W_{hd}}\\\\\n","&= \\sum_{l=1}^H V_{kl} \\frac{\\partial}{\\partial W_{hd}}\\max\\left(0, (W\\cdot X)_l\\right)\\\\\n","&= \\sum_{l=1}^H V_{kl} \\frac{\\partial \\max\\left(0, (W\\cdot X)_l\\right)}{\\partial (W\\cdot X)_l} \\cdot \\frac{\\partial \\sum_{p=1}^D W_{lp}X_p}{\\partial W_{hd}}\\\\\n","&= \\sum_{l=1}^H V_{kl}\\cdot \\mathbf{I}\\left[(W \\cdot X)_l > 0\\right] \\cdot \\sum_{p=1}^D \\delta_{lh}\\delta_{pd}X_p\\\\\n","&= V_{kh}\\cdot \\mathbf{I}\\left[(W\\cdot X)_h > 0\\right] \\cdot X_d\n","\\end{align}\n","$$\n","thus,\n","$$\n","\\begin{align}\n","\\frac{\\partial \\mathcal{L}}{\\partial W_{hd}} &= \\sum_{k=1}^C \\left(\\frac{\\exp f_k}{\\sum_{q=1}^C \\exp f_q} - \\delta_{Yk}\\right) \\cdot V_{kh} \\cdot \\mathbf{I}\\left[(W\\cdot X)_h > 0\\right] \\cdot X_d\\\\\n","&= \\mathbf{I}\\left[(W\\cdot X)_h > 0\\right] \\cdot X_d \\cdot \\sum_{k=1}^C \\left(\\frac{\\exp f_k}{\\sum_{q=1}^C \\exp f_q} - \\delta_{Yk}\\right) \\cdot V_{kh}.\n","\\end{align}\n","$$\n","Above, $\\mathbf{I}$ denotes the indicator function."]},{"cell_type":"markdown","metadata":{"id":"G0XM7OHjw97x"},"source":["## **0.2 Generalisation (5 + 5 + 5 + 5 + 5 = 25 points)**\n"]},{"cell_type":"markdown","metadata":{"id":"l-mbINjflcUA"},"source":["(a) What is the role of training, validation, and test splits of a dataset for machine learning? **(5 points)**"]},{"cell_type":"markdown","metadata":{"id":"RdJXLBo3lc60"},"source":["\n","\n","*   **Train split**: We use it to train (optimize) model parameters for a fixed hyperparameter set and method. During training, we aim to minimize the training loss (and optionally, the regularization term), typically with a variant of gradient descent.\n","*   **Validation/dev split**: We use it to choose performant hyperparameters and design choices for a fixed method, using e.g. Bayesian optimization or grid search. This already simulates deployment, as these samples are not seen by the model during training. The objective being monitored is the validation loss to measure generalization performance.\n","*   **Test split**: We use it to simulate the actual deployment scenario. The choice of the overall methodology and approach can be made based on test performance. The objective here is the test loss which also measures generalization performance.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"12uNvbjqlin6"},"source":["Consider an image classification task, where $X\\in\\mathbb{R}^D$ is an input image and $Y\\in\\{1,\\cdots,C\\}$ is the corresponding label following $p(X,Y)$. Let $\\mathcal{D}^\\text{tr}=\\{(X_i,Y_i)\\}_{i=1}^N$ and $\\mathcal{D}^\\text{te}=\\{(X_i,Y_i)\\}_{i=N+1}^{N+M}$ be training and test samples, respectively, that are IID-sampled from $p(X,Y)$. Let $f_\\theta$ be a model trained on $\\mathcal{D}^\\text{tr}$. Write $f_\\theta(c,X)\\in[0,1]$ for the predicted probability that image $X$ belongs to class $c$."]},{"cell_type":"markdown","metadata":{"id":"L528qOn1ljfI"},"source":["(b) Write down the equations for the training- and test-set accuracies of $f_\\theta$. **(5 points)**"]},{"cell_type":"markdown","metadata":{"id":"RDsZszitlnji"},"source":["Train set accuracy of $f_\\theta$:\n","$$\\text{acc}^{\\text{tr}} = \\frac{1}{N}\\sum_{i=1}^N \\delta_{Y_i,\\ \\underset{c \\in \\{1, \\dots, C\\}}{\\arg\\max} f_\\theta(c, X_i)}$$\n","Test set accuracy of $f_\\theta$:\n","$$\\text{acc}^{\\text{te}} = \\frac{1}{M}\\sum_{i=N+1}^{N+M} \\delta_{Y_i,\\ \\underset{c \\in \\{1, \\dots, C\\}}{\\arg\\max} f_\\theta(c, X_i)}$$"]},{"cell_type":"markdown","metadata":{"id":"X83E1DdjlpTR"},"source":["(c) Explain what it means to say that $f_\\theta$ \"generalises well\" and introduce a quantitative metric for this. **(5 points)**"]},{"cell_type":"markdown","metadata":{"id":"plqcogLGltw9"},"source":["$f_\\theta$ generalizes well if it adapts well to (i.e., achieves low loss on) previously unseen data samples. Typically, generalization refers to the model's performance on new samples from the same distribution as the train set, but OOD generalization measures generalization performance on samples outside the train set distribution.\n","\n","A quantitative metric for generalization performance is the **(true) risk** of the classifier over $p(X, Y)$:\n","$$R(f_\\theta) = \\mathbb{E}_{p(X, Y)}[\\ell(Y, f_\\theta(X))],$$\n","where $p(X, Y)$ is the distribution over which we want to measure generalization performance and $\\ell$ is our loss function of choice. Here,\n","$$f_\\theta(X) := \\begin{pmatrix}f_\\theta(1, X)\\\\ f_\\theta(2, X)\\\\ \\vdots\\\\ f_\\theta(C, X)\\end{pmatrix}.$$\n","For classification, it can e.g. be the cross-entropy loss from above: $\\ell(Y, f_\\theta(X)) = -\\sum_{c = 1}^C \\delta_{cY}\\log f_\\theta(c, X) = -\\log f_\\theta(Y, X)$ (here we do not apply the softmax function as it is already applied according to the task description: each entry of $f_\\theta(X)$ is normalized to [0, 1]). As this expectation is typically intractable and $p$ is usually not known closed-form, we can approximate the true risk by the **empirical risk on the test set**:\n","$$R_M(f_\\theta) = \\frac{1}{M}\\sum_{i=N+1}^{N+M}\\ell(Y_i, f_\\theta(X_i)),$$\n","where, as above, $\\mathcal{D}^\\text{te}=\\{(X_i,Y_i)\\}_{i=N+1}^{N+M}$ are test samples that are IID-sampled from $p(X,Y)$."]},{"cell_type":"markdown","metadata":{"id":"asxIvGjjlyqu"},"source":["(d) Explain the concept of overfitting and underfitting. **(5 points)**\n"]},{"cell_type":"markdown","metadata":{"id":"sfPBkF-UlzzJ"},"source":["**Underfitting** happens when our model of choice is not powerful enough to represent the true relationship between input and output. This corresponds to high bias but low variance in the bias-variance trade-off. Both the train and test errors are high.\n","\n","**Overfitting** happens when our model is powerful enough to represent the true relationship between input and output but it is also fit to noise in the train set that harms its generalization performance. This corresponds to high variance but low bias in the bias-variance trade-off. The train error is low but the test error is high. Recently it has also been observed that for heavily over-parameterized models **benign overfitting** can also take place, where the model is fit below the noise level (e.g. it interpolates, 0 train error) but it still manages to generalize well (also low test error)."]},{"cell_type":"markdown","metadata":{"id":"0Ma7ifHdl20n"},"source":["(e) Explain the respective solutions for overfitting and underfitting. **(5 points)**"]},{"cell_type":"markdown","metadata":{"id":"xAZchYMdl40v"},"source":["**Underfitting**: We can increase the model capacity (e.g. add more layers, neurons or increase $C$ in an SVM) in order to fit training samples better.\n","\n","**Overfitting**: We can employ regularization (e.g. use L2- or L1-regularization, dropout, or decrease $C$ in an SVM) to constrain the model from fitting noise and help it generalize better."]},{"cell_type":"markdown","metadata":{"id":"fXn5gZIWwd7X"},"source":["## **0.3 MNIST Case-study (15 + 15 + 10 + 10 = 50 points)**\n","\n","*This exercise is based on the public code at https://colab.research.google.com/github/skorch-dev/skorch/blob/master/notebooks/MNIST.ipynb (BSD 3-Clause License)*."]},{"cell_type":"markdown","metadata":{"id":"bADaHafLNDP6"},"source":["### **Note**\n","I changed the code below at two points, both of which I believe to be mistakes:\n","1.   In the Solver class,``self.model.eval()`` was not called at the evaluation step. Consequently, the dropout layer was not turned off during inference, degrading the models' performance.\n","2.   Softmax was applied in both forward functions. This is not needed, as we use the SoftmaxCrossEntropyLoss module as our loss, which includes the application of softmax.\n","\n","I also added comments to the respective changes.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2681,"status":"ok","timestamp":1666816718089,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"dqzjwU0JzFTl","outputId":"b0c28831-14b3-4711-b0aa-bea9774f32c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: scikit-learn==0.20.* in /usr/local/lib/python3.7/dist-packages (0.20.4)\n","Requirement already satisfied: skorch in /usr/local/lib/python3.7/dist-packages (0.11.0)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.20.*) (1.21.6)\n","Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.20.*) (1.7.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.8.10)\n","Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (4.64.1)\n"]}],"source":["!pip install torch scikit-learn==0.20.* skorch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":537,"status":"ok","timestamp":1666816718618,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"hPp10JADzaVq","outputId":"28a9ba7a-747b-4195-c90a-18f583f506e4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  dtype=np.int):\n"]}],"source":["from sklearn.datasets import fetch_openml\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import logging"]},{"cell_type":"markdown","metadata":{"id":"OjddOGaWzhsO"},"source":["### Loading Data\n","\n","The following code downloads MNIST. It takes 2-3 minutes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aht68ourzcik"},"outputs":[],"source":["mnist = fetch_openml('mnist_784', cache=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1666816744028,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"fRi_gw4FzeWK","outputId":"45fcab4f-48f3-4238-94ab-cd4b72556ef3"},"outputs":[{"data":{"text/plain":["(70000, 784)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["mnist.data.shape"]},{"cell_type":"markdown","metadata":{"id":"QcdCLOvyzoog"},"source":["### Preprocessing Data\n","\n","Each image of the MNIST dataset is encoded in a 784 dimensional vector, representing a 28 x 28 pixel image. Each pixel has a value between 0 and 255, corresponding to the grey-value of a pixel.<br />\n","The above ``fetch_openml`` method to load MNIST returns ```data``` and ```target``` as ```uint8``` which we convert to ```float32``` and ```int64``` respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5HOEqw3rzoL2"},"outputs":[],"source":["X = mnist.data.astype('float32')\n","y = mnist.target.astype('int64')"]},{"cell_type":"markdown","metadata":{"id":"PwRg0az8zt4b"},"source":["(*****) To avoid big weights that deal with the pixel values from between [0, 255], we scale `X` down. A commonly used range is [0, 1]."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VaRVxh_Kzulg"},"outputs":[],"source":["X /= 255.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1666816744028,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"Rm0A8CD6zxGs","outputId":"084193d9-090a-495b-88db-07ecfe76a7e4"},"outputs":[{"data":{"text/plain":["(0.0, 1.0)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["X.min(), X.max()"]},{"cell_type":"markdown","metadata":{"id":"BtrbiHPrD9yO"},"source":["### Preparing Train-Val-Test Splits (15 points)\n","\n","Let's split the given MNIST data (70000 samples) into 3 partitions: train, val, and test. Complete the function `train_test_split` **(15 points)**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UvhnvEe10wlM"},"outputs":[],"source":["def train_test_split(X, y, val_size, test_size, random_seed=None):\n","  \"\"\"\n","  Given a labelled dataset in memory, return two disjoint partitions, train and test, randomly split according to the given test_size.\n","  For replicability, make sure that the same random_seed returns the same split of the dataset.\n","\n","  Args:\n","    X: numpy.ndarray of shape (N, D) and type np.float32. N is the number of sample instances and D is the dimensionality of input features.\n","    y: numpy.ndarray of shape (N,) and type np.int64.\n","    val_size: float value between 0 and 1. Controls the ratio of val samples.\n","    test_size: float value between 0 and 1. Controls the ratio of test samples.\n","    random_seed: int or None. If it is int, set the random seed to ensure replicability. If it is None, do not set the random seed.\n","\n","  Returns:\n","    dict of\n","      X_train: numpy.ndarray of shape (N - L - M, D) and type np.float32.\n","      y_train: numpy.ndarray of shape (N - L - M,) and type np.int64.\n","      X_val: numpy.ndarray of shape (L, D) and type np.float32. L = int(N * val_size).\n","      y_val: numpy.ndarray of shape (L,) and type np.int64.\n","      X_test: numpy.ndarray of shape (M, D) and type np.float32. M = int(N * test_size).\n","      y_test: numpy.ndarray of shape (M,) and type np.int64.\n","  \"\"\"\n","  #### >>>> PUT YOUR SOLUTION HERE <<<< 15 points\n","  rng = np.random.default_rng(seed=random_seed)\n","  data = np.concatenate([X, y[:, np.newaxis]], axis=1)\n","  rng.shuffle(data, axis=0)\n","  X_shuffled = data[:, :-1].astype(np.float32)\n","  y_shuffled = data[:, -1].astype(np.int64)\n","\n","  N = X_shuffled.shape[0]\n","  L = int(N * val_size)\n","  M = int(N * test_size)\n","  \n","  X_train = X_shuffled[:N-L-M, :]\n","  y_train = y_shuffled[:N-L-M]\n","\n","  X_val = X_shuffled[N-L-M:N-M, :]\n","  y_val = y_shuffled[N-L-M:N-M]\n","\n","  X_test = X_shuffled[N-M:, :]\n","  y_test = y_shuffled[N-M:]\n","  #### >>>> END OF YOUR SOLUTION <<<<\n","  return {\n","      \"X_train\": X_train,\n","      \"y_train\": y_train,\n","      \"X_val\": X_val,\n","      \"y_val\": y_val,\n","      \"X_test\": X_test,\n","      \"y_test\": y_test,\n","  }"]},{"cell_type":"markdown","metadata":{"id":"Fll3Ze71E-46"},"source":["Define test functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upTtGa8HE7eK"},"outputs":[],"source":["def compute_diff_matrix(mat1, mat2, max_samples):\n","  \"\"\"\n","  Compute the pairwise L1 distance among the feature vectors in mat1 and mat2.\n","  Args:\n","    mat1: array of shape (N, D) where D is the feature dimension.\n","    mat2: array of shape (M, D)\n","    max_samples: for the interest of RAM usage, we may restrict the max number of samples\n","  Returns:\n","    dist_mat: array of shape (N, M) where N <- min(max_samples, N) and M <- min(max_samples, M)\n","  \"\"\"\n","  num_samples1 = min(max_samples, len(mat1))\n","  num_samples2 = min(max_samples, len(mat2))\n","  diff_matrix = np.abs(\n","      np.expand_dims(mat1[:num_samples1], axis=0)  # (1, N, D)\n","    - np.expand_dims(mat2[:num_samples2], axis=1)  # (M, 1, D)\n","  ).sum(axis=2)\n","  return diff_matrix\n","\n","\n","def train_test_split_test(X, y):\n","  print(f\"Checking train-val-test split sizes.\")\n","  data_splits = train_test_split(X, y, val_size=0.2, test_size=0.5, random_seed=None)\n","  print(len(data_splits[\"X_train\"]))\n","  assert len(data_splits[\"X_train\"]) == 21000\n","  assert len(data_splits[\"X_val\"]) == 14000\n","  assert len(data_splits[\"X_test\"]) == 35000\n","  assert len(data_splits[\"y_train\"]) == 21000\n","  assert len(data_splits[\"y_val\"]) == 14000\n","  assert len(data_splits[\"y_test\"]) == 35000\n","  data_splits = train_test_split(X, y, val_size=0.1, test_size=0.8, random_seed=None)\n","  assert len(data_splits[\"X_train\"]) == 7000\n","  assert len(data_splits[\"X_val\"]) == 7000\n","  assert len(data_splits[\"X_test\"]) == 56000\n","  assert len(data_splits[\"y_train\"]) == 7000\n","  assert len(data_splits[\"y_val\"]) == 7000\n","  assert len(data_splits[\"y_test\"]) == 56000\n","\n","  print(f\"Checking train-test split purity.\")\n","  diff_matrix_tr_te = compute_diff_matrix(mat1=data_splits[\"X_train\"],\n","                                          mat2=data_splits[\"X_test\"],\n","                                          max_samples=1000)\n","  assert diff_matrix_tr_te.min() > 0\n","  diff_matrix_tr_val = compute_diff_matrix(mat1=data_splits[\"X_train\"],\n","                                           mat2=data_splits[\"X_val\"],\n","                                           max_samples=1000)\n","  assert diff_matrix_tr_val.min() > 0\n","  diff_matrix_te_val = compute_diff_matrix(mat1=data_splits[\"X_test\"],\n","                                           mat2=data_splits[\"X_val\"],\n","                                           max_samples=1000)\n","  assert diff_matrix_te_val.min() > 0\n","\n","  print(f\"Checking whether random seed is working.\")\n","  data_splits1 = train_test_split(X, y, val_size=0.1, test_size=0.3, random_seed=65)\n","  data_splits2 = train_test_split(X, y, val_size=0.1, test_size=0.3, random_seed=65)\n","  data_splits3 = train_test_split(X, y, val_size=0.1, test_size=0.3, random_seed=66)\n","  data_splits4 = train_test_split(X, y, val_size=0.1, test_size=0.3, random_seed=None)\n","  data_splits5 = train_test_split(X, y, val_size=0.1, test_size=0.3, random_seed=None)\n","  \n","  assert (data_splits1[\"X_train\"]==data_splits2[\"X_train\"]).all()\n","  assert (data_splits2[\"X_train\"]!=data_splits3[\"X_train\"]).any()\n","  assert (data_splits4[\"X_train\"]!=data_splits5[\"X_train\"]).any()"]},{"cell_type":"markdown","metadata":{"id":"CddjJDafllCO"},"source":["Test your solution by running the test function below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSl_We9Tz0Kj"},"outputs":[],"source":["data_splits = train_test_split(X, y, val_size=0.1, test_size=0.1, random_seed=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1666816745196,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"_vTSvDQZz5dU","outputId":"fc2d1549-1cdf-435d-8f00-f777ac7f5839"},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train shape: (56000, 784)\n","y_train shape: (56000,)\n","X_val shape: (7000, 784)\n","y_val shape: (7000,)\n","X_test shape: (7000, 784)\n","y_test shape: (7000,)\n"]}],"source":["for key in data_splits:\n","  print(f\"{key} shape: {data_splits[key].shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zgYMhzTBz9N9"},"outputs":[],"source":["def plot_example(X, y, num_samples=10):\n","    \"\"\"Plot the first N images and their labels in a row.\"\"\"\n","    for i, (img, y) in enumerate(\n","        zip(X[:num_samples].reshape(num_samples, 28, 28), y[:num_samples])\n","    ):\n","        plt.subplot(1, num_samples, i+1)\n","        plt.imshow(img)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.title(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":79},"executionInfo":{"elapsed":683,"status":"ok","timestamp":1666816745876,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"6SSITy7R0ALB","outputId":"05e198ce-37e1-4806-f222-abf33a0e4221"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 10 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_example(data_splits[\"X_train\"], data_splits[\"y_train\"])"]},{"cell_type":"markdown","metadata":{"id":"5TOSzJD-06fS"},"source":["### Define an MLP Model\n","Simple, fully connected neural network with one hidden layer. Input layer has 784 dimensions (28x28), hidden layer has 98 (= 784 / 8) and output layer 10 neurons, representing digits 0 - 9."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SJhz-4606JK"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnExi2at0-9w"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1666816746311,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"AfA6jDFX4fEG","outputId":"4d981142-1ce6-420a-c5ab-8f3664624b16"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'cuda'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evkViHmF1Gyi"},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(\n","            self,\n","            input_dim,\n","            hidden_dim,\n","            output_dim,\n","            dropout=0.5,\n","    ):\n","        super(MLP, self).__init__()\n","        print(f\"input_dim={input_dim}, hidden_dim={hidden_dim}, output_dim={output_dim}\")\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.hidden = nn.Linear(input_dim, hidden_dim)\n","        self.output = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, X, **kwargs):\n","        X = F.relu(self.hidden(X))\n","        X = self.dropout(X)\n","        # The softmax activation is not needed, as it is applied in the loss\n","        X = self.output(X)\n","        return X"]},{"cell_type":"markdown","metadata":{"id":"q9ezQK5mEvvo"},"source":["### Define the Loss Function (15 points)\n","\n","We train the model with the softmax-cross-entropy loss. The first part of the function is already there.\n","```\n","outputs = outputs - outputs.max(1, keepdim=True)[0]\n","```\n","This part ensures the numerical stability of the softmax operation, which is translation invariant.\n","```\n","labels_onehot = F.one_hot(labels, num_classes=10)\n","```\n","This part turns the integer labels into one-hot vectors.\n","\n","Complete the loss function **(15 points)**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSmFhsy1Ey5d"},"outputs":[],"source":["class SoftmaxCrossEntropyLoss(nn.Module):\n","  def forward(self, outputs, labels):\n","    \"\"\"\n","    Compute softmax cross-entropy loss.\n","    Args:\n","      outputs: torch.Tensor of shape (B, C) and type float32.\n","      labels: torch.Tensor of shape (B,) and type long.\n","    Returns:\n","      loss: torch.Tensor of scalar shape and type float32.\n","    \"\"\"\n","    outputs = outputs - outputs.max(1, keepdim=True)[0]\n","    labels_onehot = F.one_hot(labels, num_classes=10)  # shape (B, C) type bool\n","\n","    #### >>>> PUT YOUR SOLUTION HERE <<<< 15 points\n","    loss = (outputs.exp().sum(dim=1).log() - (outputs * labels_onehot).sum(dim=1)).mean()\n","    #### >>>> END OF YOUR SOLUTION <<<<\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"ey7IKKQrGU1u"},"source":["### Define Solver\n","\n","A programming framework that is often used to manage model training, inference, and evaluation is the Solver object. This is advantageous because such activities around a model share common data and methods: \n","- Model (torch.nn.Module)\n","- Dataset (torch.utils.data.Dataset)\n","\n","We have defined the Solver class for you. You may find this framework useful for managing ML code later on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02p-nOmemN5k"},"outputs":[],"source":["class Solver:\n","  def __init__(self, data_splits, model, criterion, config):\n","    self.config = config\n","    self.model = model\n","    self.criterion = criterion\n","    self.optimizer = optim.SGD(self.model.parameters(),\n","                               lr=config[\"lr\"],\n","                               momentum=config[\"momentum\"])\n","    self.data_splits = data_splits\n","\n","  def load_dataloader(self, split, role):\n","    if role == \"eval\":\n","      batch_size = config[\"eval_batch_size\"]\n","      shuffle = True\n","    elif role == \"train\":\n","      batch_size = config[\"batch_size\"]\n","      shuffle = False\n","    else:\n","      raise ValueError(f\"Unknown role name {role}.\")\n","    \n","    return torch.utils.data.DataLoader(\n","      torch.utils.data.TensorDataset(\n","          torch.from_numpy(self.data_splits[f\"X_{split}\"]),\n","          torch.from_numpy(self.data_splits[f\"y_{split}\"])\n","      ), \n","      batch_size=batch_size,\n","      shuffle=shuffle,\n","      num_workers=config[\"num_workers\"]\n","    )\n","\n","  def fit_one_batch(self, inputs, labels):\n","    self.optimizer.zero_grad()\n","    outputs = self.model(inputs)\n","    loss = self.criterion(outputs, labels)\n","    loss.backward()\n","    self.optimizer.step()\n","    return loss\n","\n","  def fit_one_epoch(self, epoch_idx):\n","    self.model.train()  # Needed for activation of dropout\n","    loss_epoch = 0.0\n","    dataloader = self.load_dataloader(split=\"train\", role=\"train\")\n","    for batch_idx, (inputs, labels) in enumerate(dataloader):\n","      loss = self.fit_one_batch(inputs, labels)\n","      loss_epoch += loss.item()\n","    loss_epoch /= batch_idx + 1\n","    print(f\"train_loss {loss_epoch: 2.3f}\", end=\"\\t\")\n","\n","  def fit(self, evaluate_on):\n","    for epoch_idx in range(self.config[\"num_epochs\"]):\n","      print(f\"epoch {epoch_idx + 1:03d}\", end=\"\\t\")\n","      self.fit_one_epoch(epoch_idx)\n","      accuracy = self.evaluate(evaluate_on)\n","      print(f\"val_accuracy: {accuracy: 2.3f}%\", end=\"\\t\")\n","      print()\n","\n","  def evaluate(self, evaluate_on):\n","    self.model.eval()  # Needed for deactivation of dropout\n","    correct = 0\n","    total = 0\n","    dataloader = self.load_dataloader(split=evaluate_on, role=\"eval\")\n","    with torch.no_grad():\n","      for (inputs, labels) in dataloader:\n","        outputs = self.model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","    return 100 * correct / total"]},{"cell_type":"markdown","metadata":{"id":"t1ve3U8SHtF2"},"source":["### Train Model\n","\n","Initialise your solver with some initial config values and train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46121,"status":"ok","timestamp":1666816792426,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"7lgPmLNgm2ax","outputId":"19cd58da-8caf-47ef-9267-f0a79097818f"},"outputs":[{"name":"stdout","output_type":"stream","text":["input_dim=784, hidden_dim=98, output_dim=10\n","epoch 001\ttrain_loss  2.213\tval_accuracy:  53.529%\t\n","epoch 002\ttrain_loss  1.986\tval_accuracy:  67.957%\t\n","epoch 003\ttrain_loss  1.696\tval_accuracy:  73.429%\t\n","epoch 004\ttrain_loss  1.418\tval_accuracy:  77.129%\t\n","epoch 005\ttrain_loss  1.209\tval_accuracy:  79.700%\t\n","epoch 006\ttrain_loss  1.064\tval_accuracy:  81.443%\t\n","epoch 007\ttrain_loss  0.961\tval_accuracy:  82.971%\t\n","epoch 008\ttrain_loss  0.884\tval_accuracy:  84.229%\t\n","epoch 009\ttrain_loss  0.823\tval_accuracy:  85.114%\t\n","epoch 010\ttrain_loss  0.776\tval_accuracy:  85.757%\t\n","epoch 011\ttrain_loss  0.739\tval_accuracy:  86.529%\t\n","epoch 012\ttrain_loss  0.707\tval_accuracy:  87.043%\t\n","epoch 013\ttrain_loss  0.680\tval_accuracy:  87.543%\t\n","epoch 014\ttrain_loss  0.652\tval_accuracy:  87.814%\t\n","epoch 015\ttrain_loss  0.634\tval_accuracy:  88.071%\t\n","epoch 016\ttrain_loss  0.618\tval_accuracy:  88.471%\t\n","epoch 017\ttrain_loss  0.599\tval_accuracy:  88.700%\t\n","epoch 018\ttrain_loss  0.584\tval_accuracy:  88.800%\t\n","epoch 019\ttrain_loss  0.572\tval_accuracy:  88.986%\t\n","epoch 020\ttrain_loss  0.557\tval_accuracy:  89.143%\t\n","epoch 021\ttrain_loss  0.548\tval_accuracy:  89.271%\t\n","epoch 022\ttrain_loss  0.540\tval_accuracy:  89.429%\t\n","epoch 023\ttrain_loss  0.527\tval_accuracy:  89.543%\t\n","epoch 024\ttrain_loss  0.521\tval_accuracy:  89.657%\t\n","epoch 025\ttrain_loss  0.512\tval_accuracy:  89.757%\t\n","epoch 026\ttrain_loss  0.504\tval_accuracy:  89.900%\t\n","epoch 027\ttrain_loss  0.497\tval_accuracy:  89.914%\t\n","epoch 028\ttrain_loss  0.494\tval_accuracy:  90.043%\t\n","epoch 029\ttrain_loss  0.486\tval_accuracy:  90.014%\t\n","epoch 030\ttrain_loss  0.481\tval_accuracy:  90.043%\t\n","epoch 031\ttrain_loss  0.477\tval_accuracy:  90.171%\t\n","epoch 032\ttrain_loss  0.471\tval_accuracy:  90.171%\t\n","epoch 033\ttrain_loss  0.465\tval_accuracy:  90.271%\t\n","epoch 034\ttrain_loss  0.462\tval_accuracy:  90.371%\t\n","epoch 035\ttrain_loss  0.455\tval_accuracy:  90.543%\t\n","epoch 036\ttrain_loss  0.451\tval_accuracy:  90.600%\t\n","epoch 037\ttrain_loss  0.447\tval_accuracy:  90.614%\t\n","epoch 038\ttrain_loss  0.446\tval_accuracy:  90.671%\t\n","epoch 039\ttrain_loss  0.440\tval_accuracy:  90.700%\t\n","epoch 040\ttrain_loss  0.436\tval_accuracy:  90.843%\t\n","epoch 041\ttrain_loss  0.434\tval_accuracy:  90.900%\t\n","epoch 042\ttrain_loss  0.429\tval_accuracy:  90.914%\t\n","epoch 043\ttrain_loss  0.427\tval_accuracy:  91.014%\t\n","epoch 044\ttrain_loss  0.421\tval_accuracy:  91.014%\t\n","epoch 045\ttrain_loss  0.419\tval_accuracy:  91.114%\t\n","epoch 046\ttrain_loss  0.415\tval_accuracy:  91.129%\t\n","epoch 047\ttrain_loss  0.412\tval_accuracy:  91.157%\t\n","epoch 048\ttrain_loss  0.411\tval_accuracy:  91.229%\t\n","epoch 049\ttrain_loss  0.407\tval_accuracy:  91.286%\t\n","epoch 050\ttrain_loss  0.404\tval_accuracy:  91.329%\t\n","Final accuracy: 91.329%\n"]}],"source":["config = {\n","  # Critical configs that affect the trained model\n","  \"batch_size\": 512,\n","  \"lr\": 0.001,\n","  \"momentum\": 0.9,\n","  \"num_epochs\": 50,\n","  # Configs that do not influence the trained model\n","  \"num_workers\": 0,\n","  \"eval_batch_size\": 128,\n","}\n","\n","input_dim = data_splits[\"X_train\"].shape[1]\n","\n","solver = Solver(\n","    data_splits=data_splits,\n","    model=MLP(\n","      input_dim=input_dim,\n","      hidden_dim=int(input_dim/8),\n","      output_dim=10,\n","    ),\n","    criterion=SoftmaxCrossEntropyLoss(),\n","    config=config,\n",")\n","\n","solver.fit(evaluate_on=\"val\")\n","accuracy = solver.evaluate(evaluate_on=\"val\")\n","print(f\"Final accuracy: {accuracy:.3f}%\")"]},{"cell_type":"markdown","metadata":{"id":"VJwHPRhGD12W"},"source":["### Hyperparameter Tuning (10 points)\n","\n","Now, find a config set (`batch_size`, `lr`, `momentum`) that returns a **validation-set accuracy >= 95%** at any epoch <=50 **(10 points)**.\n","\n","**You may not change the training data.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63280,"status":"ok","timestamp":1666816855684,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"1h3m127wdMVM","outputId":"7cabcf52-5a6b-492d-8414-245c768d8fd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["input_dim=784, hidden_dim=98, output_dim=10\n","epoch 001\ttrain_loss  0.466\tval_accuracy:  94.829%\t\n","epoch 002\ttrain_loss  0.284\tval_accuracy:  95.686%\t\n","epoch 003\ttrain_loss  0.253\tval_accuracy:  96.571%\t\n","epoch 004\ttrain_loss  0.227\tval_accuracy:  96.643%\t\n","epoch 005\ttrain_loss  0.219\tval_accuracy:  96.929%\t\n","epoch 006\ttrain_loss  0.204\tval_accuracy:  96.900%\t\n","epoch 007\ttrain_loss  0.204\tval_accuracy:  97.314%\t\n","epoch 008\ttrain_loss  0.200\tval_accuracy:  97.271%\t\n","epoch 009\ttrain_loss  0.190\tval_accuracy:  97.057%\t\n","epoch 010\ttrain_loss  0.181\tval_accuracy:  97.200%\t\n","epoch 011\ttrain_loss  0.177\tval_accuracy:  97.329%\t\n","epoch 012\ttrain_loss  0.180\tval_accuracy:  97.143%\t\n","epoch 013\ttrain_loss  0.171\tval_accuracy:  97.357%\t\n","epoch 014\ttrain_loss  0.174\tval_accuracy:  97.414%\t\n","epoch 015\ttrain_loss  0.169\tval_accuracy:  97.157%\t\n","epoch 016\ttrain_loss  0.163\tval_accuracy:  97.286%\t\n","epoch 017\ttrain_loss  0.158\tval_accuracy:  97.257%\t\n","epoch 018\ttrain_loss  0.162\tval_accuracy:  97.471%\t\n","epoch 019\ttrain_loss  0.155\tval_accuracy:  97.314%\t\n","epoch 020\ttrain_loss  0.154\tval_accuracy:  97.414%\t\n","epoch 021\ttrain_loss  0.158\tval_accuracy:  97.500%\t\n","epoch 022\ttrain_loss  0.154\tval_accuracy:  97.443%\t\n","epoch 023\ttrain_loss  0.151\tval_accuracy:  97.471%\t\n","epoch 024\ttrain_loss  0.150\tval_accuracy:  97.414%\t\n","epoch 025\ttrain_loss  0.148\tval_accuracy:  97.514%\t\n","epoch 026\ttrain_loss  0.149\tval_accuracy:  97.414%\t\n","epoch 027\ttrain_loss  0.143\tval_accuracy:  97.357%\t\n","epoch 028\ttrain_loss  0.145\tval_accuracy:  97.529%\t\n","epoch 029\ttrain_loss  0.142\tval_accuracy:  97.457%\t\n","epoch 030\ttrain_loss  0.135\tval_accuracy:  97.529%\t\n","epoch 031\ttrain_loss  0.140\tval_accuracy:  97.643%\t\n","epoch 032\ttrain_loss  0.148\tval_accuracy:  97.243%\t\n","epoch 033\ttrain_loss  0.141\tval_accuracy:  97.643%\t\n","epoch 034\ttrain_loss  0.139\tval_accuracy:  97.586%\t\n","epoch 035\ttrain_loss  0.140\tval_accuracy:  97.600%\t\n","epoch 036\ttrain_loss  0.140\tval_accuracy:  97.486%\t\n","epoch 037\ttrain_loss  0.135\tval_accuracy:  97.429%\t\n","epoch 038\ttrain_loss  0.133\tval_accuracy:  97.514%\t\n","epoch 039\ttrain_loss  0.138\tval_accuracy:  97.557%\t\n","epoch 040\ttrain_loss  0.135\tval_accuracy:  97.429%\t\n","epoch 041\ttrain_loss  0.132\tval_accuracy:  97.386%\t\n","epoch 042\ttrain_loss  0.133\tval_accuracy:  97.529%\t\n","epoch 043\ttrain_loss  0.132\tval_accuracy:  97.471%\t\n","epoch 044\ttrain_loss  0.131\tval_accuracy:  97.786%\t\n","epoch 045\ttrain_loss  0.130\tval_accuracy:  97.671%\t\n","epoch 046\ttrain_loss  0.130\tval_accuracy:  97.214%\t\n","epoch 047\ttrain_loss  0.131\tval_accuracy:  97.557%\t\n","epoch 048\ttrain_loss  0.127\tval_accuracy:  97.500%\t\n","epoch 049\ttrain_loss  0.130\tval_accuracy:  97.714%\t\n","epoch 050\ttrain_loss  0.128\tval_accuracy:  97.629%\t\n","Final accuracy: 97.629%\n"]}],"source":["#### >>>> PUT YOUR SOLUTION HERE <<<< 10 points\n","config = {\n","  # Critical configs that affect the trained model\n","  \"batch_size\": 128,\n","  \"lr\": 0.1,\n","  \"momentum\": 0.9,\n","  \"num_epochs\": 50,\n","  # Configs that do not influence the trained model\n","  \"num_workers\": 0,\n","  \"eval_batch_size\": 128,\n","}\n","#### >>>> END OF YOUR SOLUTION <<<<\n","\n","# config = {\n","#   # Critical configs that affect the trained model\n","#   \"batch_size\": 512,\n","#   \"lr\": 0.07,\n","#   \"momentum\": 0.95,\n","#   \"num_epochs\": 50,\n","#   # Configs that do not influence the trained model\n","#   \"num_workers\": 0,\n","#   \"eval_batch_size\": 128,\n","# }\n","\n","input_dim = data_splits[\"X_train\"].shape[1]\n","\n","solver = Solver(\n","    data_splits=data_splits,\n","    model=MLP(\n","      input_dim=input_dim,\n","      hidden_dim=int(input_dim/8),\n","      output_dim=10,\n","    ),\n","    criterion=SoftmaxCrossEntropyLoss(),\n","    config=config\n",")\n","\n","solver.fit(evaluate_on=\"val\")\n","accuracy = solver.evaluate(evaluate_on=\"val\")\n","print(f\"Final accuracy: {accuracy:.3f}%\")"]},{"cell_type":"markdown","metadata":{"id":"9ok_1tvBIaEu"},"source":["### Define a CNN Model\n","We may take advantage of the image structure to enjoy more efficient usage of model parameters. CNNs assume that parameters can be devoted to model the composition of nearby pixels in an image, rather than any long-range dependence. We define a CNN model for you. This is a LeNet structure."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RL0AVctW1vxt"},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self, dropout=0.5):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n","        self.conv2_drop = nn.Dropout2d(p=dropout)\n","        self.fc1 = nn.Linear(1600, 100)  # 1600 = number channels * width * height\n","        self.fc2 = nn.Linear(100, 10)\n","        self.fc1_drop = nn.Dropout(p=dropout)\n","\n","    def forward(self, x):\n","        x = x.reshape(-1, 1, 28, 28)\n","        x = torch.relu(F.max_pool2d(self.conv1(x), 2))\n","        x = torch.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n","        \n","        # flatten over channel, height and width = 1600\n","        x = x.view(-1, x.size(1) * x.size(2) * x.size(3))\n","        \n","        x = torch.relu(self.fc1_drop(self.fc1(x)))\n","        # Softmax is not needed here either, it is applied in the loss\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"Oifjml3_QaG7"},"source":["### Train a CNN Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":282695,"status":"ok","timestamp":1666817138376,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"8FGYIfnO1xd2","outputId":"078cd23f-294c-4b1d-a1c4-9585e43e40fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 001\ttrain_loss  0.386\tval_accuracy:  97.600%\t\n","epoch 002\ttrain_loss  0.164\tval_accuracy:  98.086%\t\n","epoch 003\ttrain_loss  0.142\tval_accuracy:  98.357%\t\n","epoch 004\ttrain_loss  0.120\tval_accuracy:  98.514%\t\n","epoch 005\ttrain_loss  0.114\tval_accuracy:  98.486%\t\n","epoch 006\ttrain_loss  0.106\tval_accuracy:  98.586%\t\n","epoch 007\ttrain_loss  0.104\tval_accuracy:  98.757%\t\n","epoch 008\ttrain_loss  0.107\tval_accuracy:  98.700%\t\n","epoch 009\ttrain_loss  0.095\tval_accuracy:  98.729%\t\n","epoch 010\ttrain_loss  0.097\tval_accuracy:  98.871%\t\n","Final accuracy: 98.871%\n"]}],"source":["config = {\n","  # Critical configs that affect the trained model\n","  \"batch_size\": 128,\n","  \"lr\": 0.1,\n","  \"momentum\": 0.9,\n","  \"num_epochs\": 10,\n","  # Configs that do not influence the trained model\n","  \"num_workers\": 0,\n","  \"eval_batch_size\": 128,\n","}\n","\n","input_dim = data_splits[\"X_train\"].shape[1]\n","\n","solver = Solver(\n","    data_splits=data_splits,\n","    model=CNN(),\n","    criterion=SoftmaxCrossEntropyLoss(),\n","    config=config\n",")\n","\n","solver.fit(evaluate_on=\"val\")\n","accuracy = solver.evaluate(evaluate_on=\"val\")\n","print(f\"Final accuracy: {accuracy:.3f}%\")"]},{"cell_type":"markdown","metadata":{"id":"upcUH5k1L5Yc"},"source":["### Report (10 points)\n","\n","We want to answer the research question: Is MLP or CNN more advantageous for MNIST digit classification task?\n","\n","Make an argument below, based on empirical evidence, which architecture is \"better\"?\n","\n","For answering this question, consider the following aspects:\n","- Accuracy, indeed, but also\n","- Computational complexity\n","  - Space and time\n","  - Training and inference\n","- Fairness of hyperparameter tuning and model choice\n","- Error bar of accuracy\n","\n","Feel free to run additional model training experiments to support your argument. **(10 points)**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1666817138376,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"KG2_-Lze3ubL","outputId":"9aed3212-33e9-4055-96af-d2dff73eb263"},"outputs":[{"name":"stdout","output_type":"stream","text":["input_dim=784, hidden_dim=98, output_dim=10\n","Number of parameters for the MLP architecture: 77920\n","Number of parameters for the CNN architecture: 179926\n"]}],"source":["# Space\n","\n","mlp = MLP(input_dim=input_dim, hidden_dim=int(input_dim/8), output_dim=10)\n","mlp.train()\n","cnn = CNN()\n","cnn.train()\n","loss_fn = SoftmaxCrossEntropyLoss()\n","mlp_optimizer = optim.SGD(mlp.parameters(),\n","                          lr=0.1,\n","                          momentum=0.9\n",")\n","cnn_optimizer = optim.SGD(cnn.parameters(),\n","                          lr=0.1,\n","                          momentum=0.9\n",")\n","\n","def num_parameters(model):\n","    return sum(param.numel() for param in model.parameters() if param.requires_grad)\n","\n","print(f\"Number of parameters for the MLP architecture: {num_parameters(mlp)}\")\n","print(f\"Number of parameters for the CNN architecture: {num_parameters(cnn)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9vVlrn15KxO"},"outputs":[],"source":["# Time\n","\n","dummy_input = torch.randn(1, 784)\n","dummy_target = torch.tensor(2, dtype=torch.int64)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6436,"status":"ok","timestamp":1666817144808,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"V9avh0zG9nGr","outputId":"90412ada-ccda-4f97-dc51-ae744009b1e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["782 µs ± 15.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"]}],"source":["%%timeit\n","\n","dummy_output = mlp(dummy_input)\n","loss = loss_fn(dummy_output, dummy_target)\n","loss.backward()\n","mlp_optimizer.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1746,"status":"ok","timestamp":1666817146549,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"AMOl17hQ9qt3","outputId":"128435f5-66c1-472e-c8db-428a4e82a529"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.25 ms ± 74.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"]}],"source":["%%timeit\n","\n","dummy_output = cnn(dummy_input)\n","loss = loss_fn(dummy_output, dummy_target)\n","loss.backward()\n","cnn_optimizer.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQ7y8T47_zWt"},"outputs":[],"source":["mlp.eval()\n","_ = cnn.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3869,"status":"ok","timestamp":1666817150415,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"4ewJfutc_2CN","outputId":"fefd6972-31fa-4c12-9a63-e11a54c33093"},"outputs":[{"name":"stdout","output_type":"stream","text":["47.5 µs ± 1.64 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"]}],"source":["%%timeit\n","\n","with torch.no_grad():\n","    mlp(dummy_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4356,"status":"ok","timestamp":1666817154764,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"mNjBT1LX_5Pv","outputId":"5827e03e-9c54-4b3b-a025-bd53de370018"},"outputs":[{"name":"stdout","output_type":"stream","text":["520 µs ± 17.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"]}],"source":["%%timeit\n","\n","with torch.no_grad():\n","    cnn(dummy_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1666818480006,"user":{"displayName":"Bálint Mucsányi","userId":"14329416034316418738"},"user_tz":-120},"id":"JXEv_3sDHcSP","outputId":"c786b1da-be8b-456d-90aa-518f387e2dee"},"outputs":[{"name":"stdout","output_type":"stream","text":["MLP mean: 97.4486\n","MLP stddev: 0.10575934946849952\n","CNN mean: 98.76579999999998\n","CNN stddev: 0.08206680205783316\n"]}],"source":["# Accuracy error bar\n","mlp_accuracies = np.array([97.343, 97.357, 97.500, 97.414, 97.629])\n","print(\"MLP mean:\", mlp_accuracies.mean())\n","print(\"MLP stddev:\", mlp_accuracies.std())\n","\n","cnn_accuracies = np.array([98.786, 98.829, 98.657, 98.686, 98.871])\n","print(\"CNN mean:\", cnn_accuracies.mean())\n","print(\"CNN stddev:\", cnn_accuracies.std())"]},{"cell_type":"markdown","metadata":{"id":"OLhHQmChOIrL"},"source":["Disclaimer: I did not change the CNN hyperparameters because they were not wrapped in a solution block. As such, the comparison between the two architectures is not fair, as we explicitly optimized the hyperparameters for the MLP architecture, but maybe the CNN architecture does even better with tuned hyperparameters.\n","\n","Based on the hyperparameters chosen above, we can see that the final accuracies of the two models are similar, but the CNN is consistently better by more than $1\\%$, even though it was trained for only $20\\%$ of the number of epochs used for the MLP model. This is, of course, not a good measurement of generalization performance in the case of the MLP model, as I actively optimized the hyperparameters based on the validation accuracy, effectively \"spoiling\" the validation set.\n","\n","Regarding space, the CNN architecture requires more parameters. **Even though convolution layers employ weight sharing, the fully connected layers at the end of the model contain a lot of parameters that are not shared.** Considering time, the CNN architecture needs nearly 10 times more time for a single forward pass in eval mode. A full update step for a single sample requires nearly three times as much time for the CNN. We can also see from the time that a full training loop needs that the CNN is slower to train overall.\n","\n","Regarding the error bar of final accuracies, I repeated the training runs for both models 5 times. The results are:\n","$$\n","\\begin{align}\n","\\text{mlp_accuracies} &= [97.343\\%, 97.357\\%, 97.500\\%, 97.414\\%, 97.629\\%]\\\\\n","\\text{mlp_mean} &= 97.449\\%\\\\\n","\\text{mlp_error} &= 0.106\\%\\\\\n","~\\\\\n","\\text{cnn_accuracies} &= [98.786\\%, 98.829\\%, 98.657\\%, 98.686\\%, 98.871\\%]\\\\\n","\\text{cnn_mean} &= 98.766\\%\\\\\n","\\text{cnn_error} &= 0.082\\%\\\\\n","\\end{align}\n","$$\n","We can see that the final accuracies of the CNN model fluctuate less and are consistently better. Also note that the sample size (5) is very small, so the above results are coarse approximations for the true mean and std.\n","\n","Generally speaking, CNN architectures are better for image classification regarding accuracy, as they exploit translation equivariance and weight sharing. This can also be seen above: accuracy-wise the CNN model consistently outperforms the MLP model. However, for the exact architectures and hyperparameters above, the CNN model has considerably more parameters and needs a lot more time to train, without yielding notably better results. This is mainly due to the simplicity of the task (enabling good rules to be found by the MLP model) and the number of fully connected parameters in the CNN model.\n","\n","In the end, which model we choose depends on the trade-off of training and inference time and memory requirements vs. accuracy.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qsyUioUNrSV"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"1m34hoUQApiuT673oKcR97utOXFdmYRRm","timestamp":1666455067373},{"file_id":"1ku5mT26cXUN2Fl-6IvSKZEn8rott2GZJ","timestamp":1663095838877},{"file_id":"1F4vu900qzkWBre6LzJyFptIxCPXCUuM0","timestamp":1662700101318}]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.6"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
